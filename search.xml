<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/06/30/xin-xi-yu-nei-rong-an-quan-xue-zhu-lun-wen/"/>
      <url>/2022/06/30/xin-xi-yu-nei-rong-an-quan-xue-zhu-lun-wen/</url>
      
        <content type="html"><![CDATA[<h1 id="无关键点的细粒度头部姿态估计"><a href="#无关键点的细粒度头部姿态估计" class="headerlink" title="无关键点的细粒度头部姿态估计"></a>无关键点的细粒度头部姿态估计</h1><p>摘 要   人的头部姿态估计是一个关键问题，有大量的应用，如帮助注视估计，建模注意力，拟合3D模型的视频和执行脸对齐等。传统的头部姿态计算方法是通过估计目标面部的一些关键点，利用平均的头部模型求解二维到三维的对应问题。我们认为这是一个脆弱的方法，因为它完全依赖于地标检测性能，额外的头部模型和一个特别的拟合步骤。我们提出了一种优雅而稳健的方式来确定姿态，方法是在300W-LP(一个大型综合扩展数据集)上训练一个多重损失的卷积神经网络，通过联合binned姿态分类和回归，直接从图像强度预测固有欧拉角(偏航、俯仰和滚转)。我们对常见的野外姿态基准数据集进行了实证测试，显示了最先进的结果。此外，我们在通常用于使用深度进行姿态估计的数据集上测试我们的方法，并开始缩小与最先进的深度姿态方法的差距。</p><p>关键词   细粒度；头部姿态估计；多重损失；卷积神经网络；欧拉角；</p><p>Fine-Grained Head Pose Estimation Without Keypoints</p><p>Youle Wang</p><p>Department of cyber engineering, Xidian University</p><p><strong>Abstract</strong>  Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. </p><p><strong>Key words</strong>   Fine-Grained；Head Pose Estimation；multi-loss；convolutional neural network；Euler angles；</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>在过去的25年里，头部姿态估计和面部表情跟踪的相关问题在推动非刚性配准和3D重建的视觉技术以及实现操纵多媒体内容和与用户交互的新方法方面发挥了重要作用。从历史上看，有几种主要的面部建模方法，其中两种主要方法是区分/基于地标的方法[26,29]和参数化外观模型(PAMs)<a href="%E5%8F%82%E8%A7%81%5B30%5D%E8%BF%9B%E8%A1%8C%E6%9B%B4%E5%A4%9A%E7%9A%84%E8%AE%A8%E8%AE%BA">4,15</a>。近年来，利用现代深度学习工具[2,35,14]直接提取二维面部关键点的方法因其灵活性成为面部表情分析的主流方法以及对遮挡和极端姿态变化的鲁棒性。基于关键点的面部表情分析的副产品是，通过建立关键点和3D头部模型之间的对应关系并进行对齐，可以恢复头部的3D姿态。然而，在某些应用中，头部姿势可能是所有需要估计的。在这种情况下，基于关键点的方法仍然是最好的方法吗?使用现代深度学习工具还没有彻底解决这个问题，这是本文试图填补的一个文献空白。</p><p>在需要精确头部姿态估计的应用中，一个常见的解决方案是利用RGBD(深度)相机。它们可以非常准确，但也有一些限制:首先，由于它们使用主动传感，它们很难在户外和不受控制的环境情况下使用，由于主动照明可以被阳光或环境光淹没。其次，深度相机比RGB更耗电，导致在移动应用中存在严重的电池寿命问题，而且它们普遍不太流行。第三，RGBD的数据速率比RGB高，增加了存储和数据传输时间。因此，对于自动驾驶中的行人跟踪和安全监测、计算机图形学、驾驶员警觉性监测、通过视频理解社交场景等领域的广泛应用，仍然需要一种基于rgb的快速可靠的3D头部姿态估计解决方案。</p><p>我们工作的主要贡献如下:</p><p>•提出一种从图像强度直接预测头部姿态欧拉角的方法，使用多损失网络，每个角度都有损失，每个损失有两个分量:姿态分类和回归分量。我们在多个数据集的单帧姿态估计方面优于已发布的方法。</p><p>•通过在大型合成数据集上训练我们的模型，并在几个测试数据集上获得良好的结果，展示了我们的模型的泛化能力。</p><p>•介绍关于网络卷积架构的消融研究，以及我们损失函数的多个组成部分。</p><p>•详细研究了2D地标法的姿态精度，以及该方法的细节弱点，我们采用的基于外观的方法解决了这些弱点。</p><p>•研究不同方法下低分辨率对位姿估计的影响。我们表明，我们的方法结合数据增强是有效的，以解决有趣的问题，头部姿态估计低分辨率的图像。</p><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><p>人体头部姿态估计是计算机视觉研究的一个重要课题，其研究方法多种多样。在经典文献中，我们可以辨别外观模板模型(Appearance Template Models)，它试图将测试图像与一组姿势样本进行比较[17,27,28]。当正面人脸检测[18,23]的成功率越来越高时，检测器阵列曾经是一种流行的方法，其想法是训练多个人脸检测器来适应不同的头部姿势[9,34]。</p><p>最近，人脸地标探测器已经变得非常精确[2,35,14]，在姿态估计任务中很受欢迎。</p><p>最近，利用神经网络估计头部姿态的工作也得到了发展。[19]对AFLW数据集上使用回归损失训练的相对较浅网络进行了深入研究。在开普勒[14]中，作者提出了一种改进的GoogleNet架构，它可以预测面部关键点和联合姿势。他们使用来自AFLW数据集的粗姿态监督来改进地标检测。有两部作品致力于建立一个网络来完成与面部分析有关的各种预测任务。Hyperface[20]是一个CNN，开始检测面孔，确定性别，找到地标和估计头部姿势一次。它使用基于R-CNN[7]的方法和改进的AlexNet架构，融合中间卷积层输出，并添加单独的全连接网络来预测每个子任务。All-InOne人脸分析卷积神经网络[21]在原有预测任务的基础上增加了微笑、年龄估计和人脸识别。我们将我们的结果与所有这些工作进行比较。</p><p>Chang等人，[3]也支持无地标头部姿态估计。他们使用简单的CNN回归3D头部姿势，并专注于使用预测的头部姿势的面部对齐。</p><p>他们证明了他们的方法的成功，通过使用他们的面部对齐管道提高面部识别的准确性。他们不直接评估头部姿态估计结果。这与我们的工作不同，因为我们直接评估和比较我们的头部姿势结果广泛的注释数据集。</p><h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h1><p>在本节中，我们描述了直接从图像强度使用深度网络估计头部姿态的优势，并认为它应该优先于地标-姿态方法。我们解释了在更大的合成300W-LP [35]数据集上训练时，如何使用组合分类和回归来提高性能。我们还将讨论关于数据增强、训练和测试数据集的关键见解，以及如何提高低分辨率图像的性能。</p><h2 id="3-1-深度学习在头部姿态估计中的优势"><a href="#3-1-深度学习在头部姿态估计中的优势" class="headerlink" title="3.1. 深度学习在头部姿态估计中的优势"></a>3.1. 深度学习在头部姿态估计中的优势</h2><p>尽管对读者来说，经过仔细的训练，深度网络可以准确地预测图1，这似乎是显而易见的。使用我们提出的方法在困难场景下的姿态检测示例。蓝色轴指向脸的前方，绿色轴指向下方，红色轴指向侧面。彩色观看效果最佳。</p><p><img src="https://s2.loli.net/2022/06/30/6q3ucsvzW4Hmr8L.gif" alt="img"></p><p>图1</p><p>该方法尚未得到广泛的研究，也不常用于头部姿态估计任务。</p><p>相反，如果需要非常精确的头部姿势，则安装深度摄像头，如果没有深度镜头存在，则检测地标并恢复姿势。在这项工作中，我们证明了在大型合成数据集上训练的网络，根据定义具有精确的姿态注释，可以在实际情况下准确地预测姿态。我们在具有精确姿态注释的真实数据集上测试了网络，并在AFLW、AFLW2000[35]和BIWI[6]数据集上展示了最先进的结果。此外，我们开始使用非常精确的方法来缩小差距，这些方法使用BIWI数据集上的深度信息。</p><p>我们认为，与“地标-位”方法相比，深度网络有很大的优势，例如:</p><p>•它们不依赖于:选择的头部模型，地标检测方法，用于对齐头部模型的点子集或对齐2D到3D点的优化方法。</p><p>•当地标检测方法失败时，它们总是输出一个姿态预测，而后一种方法则不是这样。</p><h2 id="3-2-多重损失方法"><a href="#3-2-多重损失方法" class="headerlink" title="3.2.多重损失方法"></a>3.2.多重损失方法</h2><p>所有先前使用卷积网络预测头部姿态的工作都直接使用均方误差损失回归所有三个欧拉角。我们注意到，这种方法在我们的大规模合成训练数据上并没有取得最好的结果。</p><p>我们建议使用三个单独的损失，每个角度一个。每个损失是两个组成部分的组合:一个binned姿势分类和回归组成部分。任何骨干网都可以使用，并增加三个全连接层来预测角度。这三个完全连接的层共享之前的卷积层网络。</p><p>这种方法背后的想法是，通过执行bin分类，我们使用非常稳定的softmax层和交叉熵，因此网络学习以稳健的方式预测姿态的邻域。通过三个交叉熵损失，每个欧拉角一个，我们有三个信号反向传播到网络中，这有助于学习。为了获得一个细粒度的预测，我们计算了装箱输出的每个输出角度的期望。详细的体系结构如图2所示。</p><p><img src="https://s2.loli.net/2022/06/30/O8jXesd7nWNuqBE.gif" alt="img"></p><p>图2</p><h2 id="3-3-综合扩展数据集训练"><a href="#3-3-综合扩展数据集训练" class="headerlink" title="3.3.综合扩展数据集训练"></a>3.3.综合扩展数据集训练</h2><p>我们遵循[2]的路径，利用综合扩展的数据训练他们的地标检测模型。他们训练的数据集之一是300W-LP数据集，这是一个流行的2D里程碑数据集的集合，这些数据集已经被分组和重新注释。人脸模型适合于每一张图像，图像被扭曲以改变人脸的偏航，这给我们提供了几个偏航角度的姿势。</p><p>姿势是准确的标记，因为我们有3D模型和6-D自由度的每个图像的脸。</p><p>在4.1节中，我们展示了通过仔细训练大量的合成数据，我们可以开始弥合与现有深度方法的差距，并可以通过细粒度的姿态注释在数据集上实现非常好的准确性。我们还将我们的方法与其他深度学习方法进行了对比，这些方法的作者已经在4.1节中使用的一些测试数据集上运行了。此外，在同一节中，我们测试了地标-位姿方法和其他类型的位姿估计方法，如3D模型拟合。</p><h1 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4.实验结果"></a>4.实验结果</h1><p>我们在不同的姿态估计数据集以及流行的地标检测数据集上进行了实验，展示了我们提出的方法的整体性能。我们展示了多次损失的消融研究。此外，我们深入研究了地标姿态方法，并阐明了它们的鲁棒性。最后，我们提出的实验表明，在分辨率较低的情况下，即使地标探测器是最先进的，使用深度网络的整体姿态方法也优于地标到姿态方法。</p><h2 id="4-1-基于AFLW2000和BIWI数据集的细粒度姿态估计"><a href="#4-1-基于AFLW2000和BIWI数据集的细粒度姿态估计" class="headerlink" title="4.1. 基于AFLW2000和BIWI数据集的细粒度姿态估计"></a>4.1. 基于AFLW2000和BIWI数据集的细粒度姿态估计</h2><p>我们在AFLW2000和BIWI数据集上评估我们的方法，用于细粒度姿态估计任务。</p><p>  在AFLW2000和BIWI数据集上运行这两个地标识别器。AFLW2000图像很小，在脸部周围进行裁剪。对于BIWI，我们运行一个Faster R-CNN人脸检测器，训练于WIDER人脸数据集[32,10]，并部署在Docker容器中。为了保留头部的其余部分，我们在边框周围松散地剪裁脸部。我们还从AFLW2000的地面真实地标检索姿态。</p><p><img src="https://s2.loli.net/2022/06/30/VofEJgbyW52D7Yp.gif" alt="img"></p><p><img src="https://s2.loli.net/2022/06/30/sZCivXc3PRpF7N4.gif" alt="img"></p><p><img src="https://s2.loli.net/2022/06/30/DHRawu3SMAQpKj9.gif" alt="img"></p><p><img src="https://s2.loli.net/2022/06/30/iPvVRHgSeLW9rsq.gif" alt="img"></p><p><img src="https://s2.loli.net/2022/06/30/wxCcIpZXB96Soln.gif" alt="img"></p><p><img src="https://s2.loli.net/2022/06/30/XvZHkyCfG54RLtP.gif" alt="img"></p><p>  我们将我们的结果与最先进的RGBD方法进行比较。我们可以看到，我们提出的方法大大缩小了RGBD方法和ResNet50之间的差距。由于300W-LP数据集中缺乏大量极端基音的例子，基音估计仍然滞后。我们预计，当获得更多数据时，这一差距将会缩小。</p><h2 id="4-2-AFLW和AFW"><a href="#4-2-AFLW和AFW" class="headerlink" title="4.2.AFLW和AFW"></a>4.2.AFLW和AFW</h2><p>AFW是一个很受欢迎的数据集，也常用于测试地标检测，其中包含粗略的姿态标注。</p><p>利用AlexNet[13]的联合分类和回归损失，我们得到了25个epoch训练后相似的平均平均误差。我们将我们的结果与KEPLER[14]方法(使用改进的GoogleNet进行同时地标检测和姿态估计)和[19]方法(使用4层卷积网络)进行比较。MultiLoss ResNet50在AFLW测试集中使用Adam和4.1节中相同的学习参数进行了25个epoch的训练后，在所有角度上实现了比KEPLER更低的平均误差。</p><p>我们在AFW数据集上测试之前训练过的AlexNet和Multi-Loss ResNet50网络，并在图7中显示结果。与所有相关的工作一样，我们对偏航的结果进行了唯一的评估。我们限制我们的网络输出离散偏航在15度增量和显示精度在两个不同的偏航阈值。如果预测偏航的绝对误差小于或等于所提出的阈值，则可以正确分类。</p><p>所有比较方法采用相同的测试方案，数据直接来自相关论文。Hyperface[20]和All-In-One[21]都使用单一网络完成大量面部分析任务。Hyperface使用在ImageNet上预训练的AlexNet作为骨干，All-In-One使用在使用三重概率约束[25]的人脸识别任务上预训练的7层骨干卷积网络。</p><p>我们表明，通过在ImageNet上的预训练和在AFLW数据集上的微调，我们获得的准确性非常接近相关工作的最佳结果。我们不使用任何其他可能提高网络性能的监督信息，如2D地标注释。然而，我们确实在ResNet50中使用了一个更强大的骨干网。我们展示了同一网络在AFLW测试集和AFW上的性能。</p><h1 id="5-结论和未来工作"><a href="#5-结论和未来工作" class="headerlink" title="5.结论和未来工作"></a>5.结论和未来工作</h1><p>在这项工作中，我们表明，一个多重损失的深度网络可以直接，准确和稳健地预测头部旋转的图像强度。我们展示了这种网络优于使用最先进的地标检测方法的地标到姿态方法。本文研究了地标-姿态方法，以显示其对外部因素的依赖性。如图3所示，不同方法在低采样AFLW2000数据集上的平均平均误差，以确定方法对低分辨率图像的鲁棒性。</p><p>如头部模型和路标检测精度。</p><p>我们还表明，我们提出的方法可以推广跨数据集，它优于将头部姿态作为检测地标的子目标的网络。我们表明，在分辨率很低的情况下，地标到姿态是脆弱的，如果训练数据得到适当的增强，我们的方法对这些情况显示出鲁棒性。</p><p>对于极端姿势的合成数据生成似乎是提高所提方法性能的一种方法，对于更复杂的网络架构的研究可能会考虑到全身姿势等。</p><p><img src="https://s2.loli.net/2022/06/30/uNUTW5Vax9E7MCZ.gif" alt="img"></p><p>图3</p><p>参 考 文 献</p><p>[1] A. Bulat and G. Tzimiropoulos. Binarized convolutional</p><p>landmark localizers for human pose estimation and face</p><p>alignment with limited resources. In International Confer-</p><p>ence on Computer Vision, 2017. 5</p><p>[2] A. Bulat and G. Tzimiropoulos. How far are we from solv-</p><p>ing the 2d &amp; 3d face alignment problem? (and a dataset of</p><p>230,000 3d facial landmarks). In International Conference</p><p>on Computer Vision, 2017. 1, 2, 4, 5</p><p>[3] F.-J. Chang, A. T. Tran, T. Hassner, I. Masi, R. Nevatia, and</p><p>G. Medioni. Faceposenet: Making a case for landmark-free</p><p>face alignment. In Computer Vision Workshop (ICCVW),</p><p>2017 IEEE International Conference on, pages 1599–1608.</p><p>IEEE, 2017. 1, 2</p><p>[4] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appear-</p><p>ance models. IEEE Transactions on Pattern Analysis and</p><p>Machine Intelligence, 23(6):681–685, jun 2001. 1</p><p>[5] J. G. X. Y . S. De and M. J. Kautz. Dynamic facial analysis:</p><p>From bayesian filtering to recurrent neural network. 2017. 2,</p><p>5</p><p>[6] G. Fanelli, M. Dantone, J. Gall, A. Fossati, and L. V an Gool.</p><p>Random forests for real time 3d face analysis. Int. J. Comput.</p><p>Vision, 101(3):437–458, February 2013. 3, 5</p><p>[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-</p><p>ture hierarchies for accurate object detection and semantic</p><p>segmentation. In Computer Vision and Pattern Recognition,</p><p>\2014. 2</p><p>[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-</p><p>ing for image recognition. arXiv preprint arXiv:1512.03385,</p><p>\2015. 5</p><p>[9] J. Huang, X. Shao, and H. Wechsler. Face pose discrimina-</p><p>tion using support vector machines (svm). In Pattern Recog-</p><p>nition, 1998. Proceedings. F ourteenth International Confer-</p><p>ence on, volume 1, pages 154–156. IEEE, 1998. 2</p><p>[10] H. Jiang and E. Learned-Miller. Face detection with the</p><p>faster r-cnn. In Automatic Face &amp; Gesture Recognition (FG</p><p>2017), 2017 12th IEEE International Conference on, pages</p><p>650–657. IEEE, 2017. 5</p><p>[11] V . Kazemi and J. Sullivan. One millisecond face alignment</p><p>with an ensemble of regression trees. In Proceedings of the</p><p>IEEE Conference on Computer Vision and Pattern Recogni-</p><p>tion, pages 1867–1874, 2014. 5</p><p>[12] D. Kingma and J. Ba. Adam: A method for stochastic opti-</p><p>mization. arXiv preprint arXiv:1412.6980, 2014. 5</p><p>[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet</p><p>classification with deep convolutional neural networks. In</p><p>Advances in neural information processing systems, pages</p><p>1097–1105, 2012. 6</p><p>[14] A. Kumar, A. Alavi, and R. Chellappa. Kepler: Keypoint and</p><p>pose estimation of unconstrained faces by learning efficient</p><p>h-cnn regressors. In Automatic Face &amp; Gesture Recognition</p><p>(FG 2017), 2017 12th IEEE International Conference on,</p><p>pages 258–265. IEEE, 2017. 1, 2, 5, 6, 7, 8</p><p> [15] I. Matthews and S. Baker. Active Appearance Models Revis-</p><p>ited. International Journal of Computer Vision, 60(2):135–</p><p>164, 2004. 1</p><p>[16] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-</p><p>works for human pose estimation. In European Conference</p><p>on Computer Vision, pages 483–499. Springer, 2016. 5</p><p>[17] J. Ng and S. Gong. Composite support vector machines for</p><p>detection of faces across views and pose estimation. Image</p><p>and Vision Computing, 20(5):359–368, 2002. 2</p><p>[18] E. Osuna, R. Freund, and F. Girosit. Training support vec-</p><p>tor machines: an application to face detection. In Computer</p><p>vision and pattern recognition, 1997. Proceedings., 1997</p><p>IEEE computer society conference on, pages 130–136. IEEE,</p><p>\1997. 2</p><p>[19] M. Patacchiola and A. Cangelosi. Head pose estimation in</p><p>the wild using convolutional neural networks and adaptive</p><p>gradient methods. Pattern Recognition, 2017. 1, 2, 6, 7</p><p>[20] R. Ranjan, V . M. Patel, and R. Chellappa. Hyperface: A deep</p><p>multi-task learning framework for face detection, landmark</p><p>localization, pose estimation, and gender recognition. arXiv</p><p>preprint arXiv:1603.01249, 2016. 1, 2, 6, 8</p><p>[21] R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chel-</p><p>lappa. An all-in-one convolutional neural network for face</p><p>analysis. In Automatic Face &amp; Gesture Recognition (FG</p><p>2017), 2017 12th IEEE International Conference on, pages</p><p>17–24. IEEE, 2017. 1, 2, 6, 8</p><p>[22] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-</p><p>wards real-time object detection with region proposal net-</p><p>works. In Advances in Neural Information Processing Sys-</p><p>tems (NIPS), 2015. 5</p><p>[23] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-</p><p>based face detection. IEEE Transactions on pattern analysis</p><p>and machine intelligence, 20(1):23–38, 1998. 2</p><p>[24] N. Ruiz and J. M. Rehg. Dockerface: an easy to install and</p><p>use faster r-cnn face detector in a docker container. arXiv</p><p>preprint arXiv:1708.04370, 2017. 5</p><p>[25] S. Sankaranarayanan, A. Alavi, C. D. Castillo, and R. Chel-</p><p>lappa. Triplet probabilistic embedding for face verification</p><p>and clustering. In Biometrics Theory, Applications and Sys-</p><p>tems (BTAS), 2016 IEEE 8th International Conference on,</p><p>pages 1–8. IEEE, 2016. 6</p><p>[26] J. M. Saragih, S. Lucey, and J. F. Cohn. Deformable model</p><p>fitting by regularized landmark mean-shift. International</p><p>Journal of Computer Vision, 91(2):200–215, 2011. 1</p><p>[27] J. Sherrah, S. Gong, and E.-J. Ong. Understanding pose dis-</p><p>crimination in similarity space. In BMVC, pages 1–10, 1999.</p><p>2</p><p>[28] J. Sherrah, S. Gong, and E.-J. Ong. Face distributions in</p><p>similarity space under varying head pose. Image and Vision</p><p>Computing, 19(12):807–819, 2001. 2</p><p>[29] Xiangxin Zhu and D. Ramanan. Face detection, pose estima-</p><p>tion, and landmark localization in the wild. In Proceedings</p><p>IEEE Conference on Computer Vision and Pattern Recogni-</p><p>tion (CVPR), pages 2879–2886, jun 2012. 1</p><p>[30] X. Xiong and F. De la Torre. Supervised descent method</p><p>and its applications to face alignment. In Proceedings of the</p><p>IEEE Conference on Computer Vision and Pattern Recogni-</p><p>tion (CVPR), pages 532–539, 2013. 1</p><p>[31] H. Yang, W. Mou, Y . Zhang, I. Patras, H. Gunes, and</p><p>P . Robinson. Face alignment assisted by head pose estima-</p><p>tion. In Proceedings of the British Machine Vision Confer-</p><p>ence (BMVC), 2015. 1</p><p>[32] S. Yang, P . Luo, C. C. Loy, and X. Tang. Wider face: A</p><p>face detection benchmark. In IEEE Conference on Computer</p><p>Vision and Pattern Recognition (CVPR), 2016. 5</p><p>[33] Y . Y u, K. A. F. Mora, and J. M. Odobez. Robust and accu-</p><p>rate 3d head pose estimation through 3dmm and online head</p><p>model reconstruction. In Automatic Face &amp; Gesture Recog-</p><p>nition (FG 2017), 2017 12th IEEE International Conference</p><p>on, pages 711–718. IEEE, 2017. 5</p><p>[34] Z. Zhang, Y . Hu, M. Liu, and T. Huang. Head pose</p><p>estimation in seminar room using multi view face detec-</p><p>tors. In International Evaluation Workshop on Classifica-</p><p>tion of Events, Activities and Relationships, pages 299–304.</p><p>Springer, 2006. 2</p><p>[35] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li. Face alignment</p><p>across large poses: A 3d solution. In Proceedings of the</p><p>IEEE Conference on Computer Vision and Pattern Recogni-</p><p>tion, pages 146–155, 2016. 1, 2, 3, 5</p><p>[36] X. Zhu and D. Ramanan. Face detection, pose estimation,</p><p>and landmark localization in the wild. In Computer Vision</p><p>and Pattern Recognition (CVPR), 2012 IEEE Conference on,</p><p>pages 2879–2886. IEEE, 2012. 8</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/30/danielwang1314-readme/"/>
      <url>/2022/06/30/danielwang1314-readme/</url>
      
        <content type="html"><![CDATA[<p>作为西电网信院的一个学生，大学时光里产出了很多课程的实验报告或者作业什么的，想着能在这里分享一下，或许会对某些人有所帮助</p><p>会陆陆续续上传一些内容，包括论文大作业，实验报告，实验代码，心得体会什么的</p><p>就当是想稍微摆烂时的一种放松吧！</p><p><img src="https://s2.loli.net/2022/06/30/HGz51YVKgNMRt7L.png" alt="image-20220630141653034"></p><p><img src="https://s2.loli.net/2022/06/30/37aMlwgf16kGbPu.png" alt="image-20220630141714516"></p><p>alone but not lonely</p><p>开心快乐最重要~~~</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/30/ren-gong-zhi-neng-shi-yan-readme/"/>
      <url>/2022/06/30/ren-gong-zhi-neng-shi-yan-readme/</url>
      
        <content type="html"><![CDATA[<h1 id="网信院人工智能实验"><a href="#网信院人工智能实验" class="headerlink" title="网信院人工智能实验"></a>网信院人工智能实验</h1><p>使用时将数据集和python源代码放在一起即可</p><p>实验验收具体可能会问的问题：</p><p>1 让你讲一讲模型</p><p>2 留出法的原理</p><p>3 k折交叉验证法的原理</p><p>4 留出法和k折交叉验证法对于数据集划分比例不同对结果的影响</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/06/29/ren-gong-zhi-neng-ke-cheng-shi-yan-bao-gao/"/>
      <url>/2022/06/29/ren-gong-zhi-neng-ke-cheng-shi-yan-bao-gao/</url>
      
        <content type="html"><![CDATA[<h1 id="《人工智能》课程"><a href="#《人工智能》课程" class="headerlink" title="《人工智能》课程"></a>《人工智能》课程</h1><h1 id="实验报告"><a href="#实验报告" class="headerlink" title="实验报告"></a>实验报告</h1><p>​                                    <img src="https://s2.loli.net/2022/06/30/qrbMW1zJNuDOIAP.jpg">  </p><p>​                                                            </p><p>​    </p><h1 id="网络与信息安全学院"><a href="#网络与信息安全学院" class="headerlink" title="网络与信息安全学院"></a><strong>网络与信息安全学院</strong></h1><h1 id="班-级："><a href="#班-级：" class="headerlink" title="班  级："></a><strong>班  级：</strong></h1><h1 id="姓-名："><a href="#姓-名：" class="headerlink" title="姓  名："></a><strong>姓  名：</strong></h1><h1 id="学-号："><a href="#学-号：" class="headerlink" title="学  号："></a><strong>学  号：</strong></h1><h1 id="提交时间：-2022-x2F-6-x2F-25"><a href="#提交时间：-2022-x2F-6-x2F-25" class="headerlink" title="提交时间： 2022/6/25"></a><strong>提交时间：</strong> 2022/6/25</h1><h2 id="基于神经网络的MNIST手写数字识别"><a href="#基于神经网络的MNIST手写数字识别" class="headerlink" title="基于神经网络的MNIST手写数字识别"></a><strong>基于神经网络的MNIST手写数字识别</strong></h2><p><strong>一、实验目的</strong></p><p>w  掌握运用神经网络模型解决有监督学习问题</p><p>w  掌握机器学习中常用的模型训练测试方法</p><p>w  了解不同训练方法的选择对测试结果的影响</p><p><strong>二、****实验内容</strong></p><p><strong>MNIST****数据集</strong></p><p>本实验采用的数据集MNIST是一个手写数字图片数据集，共包含图像和对应的标签。数据集中所有图片都是28x28像素大小，且所有的图像都经过了适当的处理使得数字位于图片的中心位置。MNIST数据集使用二进制方式存储。图片数据中每个图片为一个长度为784（28x28x1，即长宽28像素的单通道灰度图）的一维向量，而标签数据中每个标签均为长度为10的一维向量。</p><p><strong>分层采样方法</strong></p><p>分层采样（或分层抽样，也叫类型抽样）方法，是将总体样本分成多个类别，再分别在每个类别中进行采样的方法。通过划分类别，采样出的样本的类型分布和总体样本相似，并且更具有代表性。在本实验中，MNIST数据集为手写数字集，有0~9共10种数字，进行分层采样时先将数据集按数字分为10类，再按同样的方式分别进行采样。</p><p><strong>神经网络模型评估方法</strong></p><p>​    通常，我们可以通过实验测试来对神经网络模型的误差进行评估。为此，需要使用一个测试集来测试模型对新样本的判别能力，然后以此测试集上的测试误差作为误差的近似值。两种常见的划分训练集和测试集的方法：</p><p>​    留出法（hold-out）直接将数据集按比例划分为两个互斥的集合。划分时为尽可能保持数据分布的一致性，可以采用分层采样（stratified sampling）的方式，使得训练集和测试集中的类别比例尽可能相似。需要注意的是，测试集在整个数据集上的分布如果不够均匀还可能引入额外的偏差，所以单次使用留出法得到的估计结果往往不够稳定可靠。在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。</p><p>​    k折交叉验证法（k-fold cross validation）先将数据集划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即也采用分层采样（stratified sampling）的方法。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练集和测试集，从而可以进行k次训练和测试。最终返回的是这k个测试结果的均值。显然，k折交叉验证法的评估结果的稳定性和保真性在很大程度上取决于k的取值。k最常用的取值是10，此外常用的取值还有5、20等。</p><p>三、实验方法设计</p><p>​    介绍实验中程序的总体设计方案、关键步骤的编程方法及思路，主要包括:</p><p>1） 模型构建的程序设计（伪代码或源代码截图）及说明解释 </p><p><img src="https://s2.loli.net/2022/06/30/WHi7zBohXSglEJL.png" alt="人工智能实验"></p><p>构建上图所示神经网络为模型，该神经网络包含4个隐藏层，4个隐藏层中的神经元数量分别为200、100、60、30, 层与层之间使用的激活函数为relu激活函，设置激活函数的目的是保证结果输出的非线性化，RELU激活函数需要大于一个阈值，才会有输出，和人的神经元结构很像。由最后一个隐藏层到输出层的激活函数为softmax function，，呈逐层递减的趋势</p><p><strong>代码：</strong></p><p># 构建和训练模型</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_and_test</span><span class="token punctuation">(</span>images_train<span class="token punctuation">,</span> labels_train<span class="token punctuation">,</span> images_test<span class="token punctuation">,</span> labels_test<span class="token punctuation">,</span> images_validation<span class="token punctuation">,</span> labels_validation<span class="token punctuation">)</span><span class="token punctuation">:</span>  X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  y_ <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  L <span class="token operator">=</span> <span class="token number">200</span>  M <span class="token operator">=</span> <span class="token number">100</span>  N <span class="token operator">=</span> <span class="token number">60</span>  O <span class="token operator">=</span> <span class="token number">30</span>  W1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">784</span><span class="token punctuation">,</span> L<span class="token punctuation">]</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 784 = 28 * 28</span>  B1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>L<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span>   W2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>L<span class="token punctuation">,</span> M<span class="token punctuation">]</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  B2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span>   W3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>M<span class="token punctuation">,</span> N<span class="token punctuation">]</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  B3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span>   W4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>N<span class="token punctuation">,</span> O<span class="token punctuation">]</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  B4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>O<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span>   W5 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span>O<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   B5 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  XX <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   Y1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>XX<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> B1<span class="token punctuation">)</span>  Y2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Y1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> B2<span class="token punctuation">)</span>  Y3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Y2<span class="token punctuation">,</span> W3<span class="token punctuation">)</span> <span class="token operator">+</span> B3<span class="token punctuation">)</span>  Y4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Y3<span class="token punctuation">,</span> W4<span class="token punctuation">)</span> <span class="token operator">+</span> B4<span class="token punctuation">)</span>  Ylogits <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Y4<span class="token punctuation">,</span> W5<span class="token punctuation">)</span> <span class="token operator">+</span> B5  Y <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>Ylogits<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2） 模型迭代训练的程序设计（伪代码或源代码截图）及说明解释 </p><p>损失函数即交叉熵损失函数：</p><p><img src="https://s2.loli.net/2022/06/30/1Iziw4aljmbVCvK.jpg" alt="img"></p><p>设置学习率：0.1</p><p><img src="https://s2.loli.net/2022/06/30/GSFnWaclYNyievr.jpg" alt="img"></p><p>设置优化器，梯度下降，使损失函数最小：</p><p><img src="https://s2.loli.net/2022/06/30/7cWSul3vgjRw8ma.jpg" alt="img"></p><p>然后进行迭代循环：</p><p><img src="https://s2.loli.net/2022/06/30/eS5BVaivhbPKOMu.jpg" alt="img"></p><p>3） 模型训练过程中周期性测试的程序设计（伪代码或源代码截图）及说明解释（周期性测试指的是每训练n个step就对模型进行一次测试，得到准确率和loss值）</p><p>设置i=5000，即设置迭代次数为5000，且每隔100次迭代输出一次损失函数值和模型在验证集上的准确率。</p><p>代码如上图所示：</p><p><img src="https://s2.loli.net/2022/06/30/fZmx9o1ndHcLkp6.jpg" alt="img"></p><p>4） 分层采样的程序设计（伪代码或源代码截图）及说明解释 </p><p>使用sklearn库model_selection模块中的train_test_split模块</p><p><img src="https://s2.loli.net/2022/06/30/XuC4Blf1md73x2V.jpg" alt="img"></p><p>并使用留出法对数据集进行划分，划分之后再调用前面已经写好的train_and_test函数进行模型训练以及测试。</p><p>“留出法”直接将数据集划分为两个互斥的集合，其中一个作为训练集，另外一个作为测试集，在训练集上训练模型，使用测试集来评估测试误差，作为对泛化误差的估计。</p><p><img src="https://s2.loli.net/2022/06/30/PeZovKs7b29LAim.jpg" alt="img"></p><p>5） k折交叉验证法的程序cc设计（伪代码或源代码截图）及说明解释 </p><p>k折交叉验证法（k-fold cross validation）先将数据集划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即也采用分层采样（stratified sampling）的方法。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练集和测试集，从而可以进行k次训练和测试。最终返回的是这k个测试结果的均值。</p><p>具体操作为：</p><p>1、 将全部训练集 S分成 k个不相交的子集，假设 S中的训练样例个数为 m，那么每一个子 集有 m/k 个训练样例，，相应的子集称作 {s1,s2,…,sk}。</p><p>2、每次从分好的子集中里面，拿出一个作为测试集，其它k-1个作为训练集</p><p>3、根据训练训练出模型或者假设函数。</p><p>4、 把这个模型放到测试集上，得到分类率。</p><p>5、计算k次求得的分类率的平均值，作为该模型或者假设函数的真实分类率。</p><p>模型图为：</p><p><img src="https://s2.loli.net/2022/06/30/4AgmbCzWthL9UXk.png" alt="IMG_256"></p><p>代码实现：</p><p><img src="https://s2.loli.net/2022/06/30/57FuPidYGAyhWXQ.jpg" alt="img"></p><p><strong>四、实验结果展示</strong></p><p>展示程序界面设计、运行结果及相关分析等，主要包括：</p><p>1）模型在验证集下的准确率（输出结果并截图）</p><p><img src="https://s2.loli.net/2022/06/30/KFg1yJQqBLnW7f5.png" alt="image-20220630104230347"></p><p><img src="https://s2.loli.net/2022/06/30/URKNuqradLeslcv.png" alt="image-20220630104243394"></p><p><img src="https://s2.loli.net/2022/06/30/oLc5sdkXJO2zfia.png" alt="image-20220630104251250"></p><p><img src="https://s2.loli.net/2022/06/30/vC7YM4Wc8rLZIVg.jpg" alt="N~O76~_P1RO@$OA)XYHPB_F"></p><p>由上图看出，叠代3700次的时候，已经满足了准确率达到97%的需求。</p><p>2）不同模型参数（隐藏层数、隐藏层节点数）对准确率的影响和分析 （10分）</p><p>不同隐藏层数对于准确率的影响：</p><table><thead><tr><th>隐藏层数量</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.8978</td><td>0.9316</td><td>0.9424</td><td>0.9446</td><td>0.9514</td></tr></tbody></table><p>隐藏层数越多，模型准确率越高。猜测隐藏层数越多，可挖掘到的数据中的特征就越全面，进而使准确率越高。</p><p>不同隐藏层节点数对准确率的影响，为了方便测试，这里选用仅有一层隐藏层的神经网络模型：</p><table><thead><tr><th>隐藏层节点数</th><th>100</th><th>200</th><th>300</th><th>400</th><th>500</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.9306</td><td>0.9316</td><td>0.9378</td><td>0.9386</td><td>0.9496</td></tr></tbody></table><p>不难看出，隐藏层节点数越多，模型准确率越高。同理，隐藏层的节点越多，可挖掘到的数据中的特征就越全面，进而使准确率越高。</p><p>3）不同训练参数（batch size、epoch num、学习率）对准确率的影响和分析 （10分）</p><p>Batch size对准确率的影响：</p><table><thead><tr><th>Batch size</th><th>50</th><th>100</th><th>150</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.9394</td><td>0.9514</td><td>0.9484</td></tr></tbody></table><p>这里无法从上述几个数据中直接看出batch size对准确率的影响，于是我上网查找相关资料显示：batch-size：顾名思义就是批次大小，也就是一次训练选取的样本个数。一般来说，在合理的范围之内，越大的 batch size 使下降方向越准确，震荡越小；batch size 如果过大，则可能会出现局部最优的情况；小的 batch size 引入的随机性更大，难以达到收敛，极少数情况下可能会效果变好。</p><p>对于本实验来说，100为最优的batch size值。</p><p>Epoch num对准确率的影响：</p><table><thead><tr><th>Epoch num</th><th>15</th><th>25</th><th>35</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.9474</td><td>0.9514</td><td>0.9488</td></tr></tbody></table><p>也不能直观地看出epoch num对于模型准确率的影响。1个epoch指用训练集中的全部样本训练一次，此时相当于batchsize 等于训练集的样本数。对于不同的数据集，答案是不一样的。但是数据的多样性会影响合适的epoch的数量。</p><p>对于本实验来说，25为最优的epoch num值。</p><p>学习率对准确率的影响：</p><table><thead><tr><th>学习率</th><th>0.01</th><th>0.05</th><th>0.1</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.8478</td><td>0.9324</td><td>0.9514</td></tr></tbody></table><p>学习率越高，则初期训练过程越快，模型的准确率越高。但该结果仅仅是由于我们选取的准确率测试节点为1000次迭代之后，或是我没有对更高的学习率情况进行测试。随着学习率的增加，损失会慢慢变小，而后增加，而最佳的学习率就可以从其中损失最小的区域选择。有经验的工程人员常常根据自己的经验进行选择，比如0.1，0.01等。随着学习率的增加，模型也可能会从欠拟合过度到过拟合状态。</p><p>对于本实验来说，0.1为最优的学习率。</p><p>留出法不同比例对结果的影响和分析 （10分）</p><table><thead><tr><th>训练集所占比例</th><th>0.6</th><th>0.7</th><th>0.8</th><th>0.9</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.7927</td><td>0.7769</td><td>0.7957</td><td>0.8033</td></tr></tbody></table><p>从上述实验中无法直观看出留出法的不同比例对于准确率的影响。留出法的训练集所占比对于模型准确率的影响是不稳定的。</p><p>k折交叉验证法不同k值对结果的影响和分析 （10分）</p><table><thead><tr><th>k</th><th>5</th><th>10</th><th>20</th></tr></thead><tbody><tr><td>1000次迭代后的模型准确率</td><td>0.9386</td><td>0.9514</td><td>0.9216</td></tr></tbody></table><p>k值越大，偏差越小，方差越大，过拟合的可能性越大；k值越小，偏差越大，方差越小，模型欠拟合的可能性越大。总的来说，如果选取交叉验证法，需要选取合适的k值，才能使准确率更高。</p><p>从最终的结果来看，对于本实验来说，k=10为最优的k值。</p><p><strong>五、****实验总结及心得</strong></p><p>  从本次实验中，我学会了如何构建一个训练模型，并掌握了留出法和k折交叉验证法的原理，实现了对MNIST手写数字的识别。</p><p>在整个实验过程中，大家的模型构建可能都是大同小异的，但是从不同的训练参数中分析总结过程是很宝贵的。一个模型的好与坏很大程度上取决于所选择的参数是否合理，因此，不仅需要重视模型构建的方法，还需要体验调参过程、总结调参经验。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/06/29/hello-world/"/>
      <url>/2022/06/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
